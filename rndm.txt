session:

	- we are not actually using the text that we filetered in the feature analysis notebook to pretrain llama for the generated text

	- llama does really bad - dataset was very small. mention in mitigation

	- for the appendix - include explanations of the ling features - data descriptions, licenses of the data, -theory ml stuff, - all the tech stuff we included - parameter choices, - graphs, raw table of all the results, - cross validation performance in graphs

	

Alberto rndm stuff:



	finish the actual work before writing phase



- scaling whole tables, not separated

- doing the eda for the second dataset to explain why the models are not performing that well

- if I have time: do correlation matrices for feature - polarity



	for complementary materials


- export plots for the ling features in BOTH datasets

- plot the crossvalidation for the models

- export the tables with the performance of the models on the testing data v1

- export the tables with the performance of the models on the 2nd dataset v2

- use plotly or tableau?



	for writing the report


- feature engineering process, use proper terminology. add feature engineering section to the data stuff. Feature Construction/transformation is what we did with the TCT


- explain the unbalanced dataset - some feature values are very heavily skewed fake or true


- future work section:	- social network for the generation of fake news to ensure the news are fake fake and there are not incidental true news.

			- scalability of doing all the models for the data. we only had 480 rows (pieces of text) with 64 columns, doin the same for a larger dataset 			  could be more costly

			- relying on textual fts means that as generative models are able to generate better texts, trad models may perform way worse and we would need 			  much more data. 
			
			- subgenre of fake news detection: ai/llm generated detection? we can all tell when a text is ai generated, certain ling characteristics that 			  define ai generated text



- results discussion:	- use the ml models intuition that queen was doing to explain why some (random forest, k-neighbors) were doing better than others

			- explain insights from the cross-validation

			- use the ling eda + ml model intuition to explain why 2nd the dataset detection is way worse

			- 

			- 

			- 


	at the end


- clean up the notebooks

- properly organize the github, remove duplicates

- write the readme of the github
